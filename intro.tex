At the core of this dissertation is a novel technique for implementing call-by-need
semantics. Call-by-need semantics evaluates arguments at call sites \emph{at most
once}. In this way, it combines the advantages of call-by-value which evaluates
arguments exactly once, and call-by-name, which evaluates arguments zero or more
times. The primary insight of this dissertation is that we can implement this
semantics in a natural way using a well known structure in programming language
implementation: the shared environment. 

\section{Motivation}

Like the dissertation as a whole, the motivation is two-fold. First, existing
call-by-need implementations are more eager than necessary about constructing
efficient argument closures. This implies that any under-used argument closure
will have wasted work. Part of the reason this has stayed the case is that it is
not obvious there is a better way. We discuss this motivation in more detail in
Chapter~\ref{chap:cem}. 

The second motivation is that existing call-by-need implementations are complex
in nature, due to their use of supercombinators and flat environments. The STG
machine in particular incorporates many clever techniques for avoiding overheads
of lazy evaluations. While this is useful for generating fast code, it also
makes formal reasoning about the compiler more difficult. This is likely the
primary reason that there aren't existing verified compilers for call-by-need at
the time of this dissertation.

\section{Overview}

This dissertation can be thought of as an exploration of the advantages of
implementing call-by-need with shared environments. There are two advantages to
this approach, corresponding directly to the two motivation points above. First,
it minimizes overheads for thunk creation. It, in a sense, is \emph{lazier}
about lazy evaluation. I take advantage of this by creating a complete native
code compiler for a simple functional language, and showing it has good
performance despite an exceedingly simple implementation. Second, it lends
itself to a exceedingly simple implementation. The simplicity of implementation
allowed investigating some interesting optimizations and implementation
approaches, as well as comparisons with existing implementation approaches, with
only one person doing the development work. Moreover, I show how the simplicity
lets us reason formally to verify the correctness of the implementation.

\section{Call-by-Need}

Because this work is focused on implementing call-by-need semantics, it is worth
spending some time discussing why call-by-need semantics are important. One easy
argument is that call-by-need underlies the widely used programming language
Haskell. Technically, Haskell is a non-strict language. This implies that both
call-by-name and call-by-need are valid implementation strategies. In practice,
there are some situations when one would prefer call-by-name, namely, when
storing an intermediate value is more expensive than re-computing it. These
cases are not limited to call-by-need; call-by-value suffers the same issue.
This implies that in theory, Haskell could switch between call-by-name and
call-by-value depending on the situation. In practice, GHC effectively always
chooses call-by-need, sometimes performing transformations that increase
memoization \cite{jones96floating}.

Even amongst the Haskell community, the advantages and disadvantages of
non-strict evaluation are hotly debated. For example, there exist both
strictness annotations, and even strict-by-default variants of Haskell. There are
real reasons for preferring strict evaluation in some contexts. For example,
memory leaks due to naive attempts at freeing memory are a common issue with
lazy semantics. 

The advantages of non-strict semantics often show up when attempting to write
high level, higher-order abstractions. An example of this principle can be found
in the \texttt{lens} Haskell library. By using laziness everywhere possible, it
ensures that code can compose lenses without introducing nontermination. This is
a strong argument for code re-use advantages in non-strict languages: by using
laziness, one can prevent nontermination everywhere possible without any
additional work. 

In general, the purpose of this dissertation is not to convince the reader that
call-by-need is the ideal semantics. Instead, it takes as a given that it is one
worth spending time on. Finally, I will say that I believe there is a lot of
interesting potential for better understanding the time and space behavior of
non-strict languages. From substructural types to dependent types to control
flow analyses, the field of reasoning about time and space requirements for
higher order languages is still in its infancy, and I am excited to see it
combined with work like that presented here to give developers high confidence
in both their programs' correctness and their time and space requirements. 

\section{Retrospective}

As with any sufficiently large effort, there are places where, if one were to
start over, things would be done differently. An obvious candidate for this
re-writing of history for this dissertation would be \emph{starting} with a
verified compiler, so that the native code compiler itself would be verified.
While noble, this would be a daunting task. Implementing a full native code
compiler is a challenge in itself, but specifying, implementing, and verifying a
compiler, given the current state of verification tools, would be a daunting
undertaking. That said, it would likely have worked to verify and export into
Haskell fragments of the native code compiler. While this would certainly have
been feasible, it would have made modification more difficult. For example,
multiple times through the implementation process, the core language was
extended. Making such core changes in the presence of proofs of correctness
would make for a painful process, something that would have slowed down valuable
time experimenting with the implementation.

\section{Contributions}

There are two primary contributions of my thesis, embodied in the following two
artifacts:

\begin{itemize}
\item A full native code compiler from a simple lazy functional language with
literals and primitive operations to x86\_64 machine code. We show that the
compiler performs comparably to the state of the art on a number of benchmarks.
This implementation and analysis provide evidence supporting the thesis that
shared-environment call-by-need has efficiency benefits over existing
approaches.

\item A verified compiler that compiles from lambda calculus to a simple
instruction machine, along with a specification of correctness and a proof that
the compiler adheres to that specification. The compiler is implemented and the
proofs checked in Coq. This is the first verified compiler of a call-by-need
semantics. This implementation and mechanized proof provides evidence for the
thesis that the simple compiler artifact that arises from shared-environment
call-by-need has benefits for formal reasoning.
\end{itemize}

Combined, these contributions support the core thesis of this dissertation: that
shared-environment call-by-need has valuable contributions to make to the study
and implementation of call-by-need compilers.
