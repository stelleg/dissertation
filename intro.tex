The strength of lazy functional programming languages is the freedom they give
the programmer to focus on correctness instead of operational details. In a
strict language, the programmer specifies what code will run, and when. In a
lazy language, the programmer only specifies what the result should be, leaving
the compiler responsible for ensuring that only code that is needed will be
executed. Thanks to this freedom from operational concerns, there are two
properties that lazy functional programmers tend to have. First, they reason
about the correctness of their code to a degree seen almost nowhere else in the
programming community~\cite{hughes1989functional, spector2018total}.  Second,
they rely on compilers to generate efficient code in a way that programmers of
strict languages don't. Essentially, they are leaving more operational decisions
up to the compiler, and focusing their energy more on the correctness of their
code. To paraphrase John Hughes, laziness is an essential tool for modular
programming. As anyone who has done formal reasoning about programs knows,
modular programming is essential for reasoning about programs. 

It is for this reason that we compiler implementors must take great care in the
design of our compilers for lazy languages. We must build compilers that
generate efficient code: our programmers are relying particularly heavily on
our ability to generate efficient code. We also must ensure that our compilers
are correct: because lazy functional programmers are free to reason about the
correctness of their code, we must ensure that any additional reasoning is not
invalidated by bugs in the compiler. To illustrate this point: if a lazy
functional programmer has time to prove ten theorems about his programs, while
the strict language programmer only has time to prove three, then a bug in the
lazy compiler may invalidate ten theorems, while a bug in the strict compiler
may only invalidate three.

This dissertation presents a tool for attaining these two goals: a novel
technique for implementing lazy semantics using shared environments, formalized
as the \ce machine.  Essentially, the \ce machine repurposes shared environments
to share the results of computation. The thesis of this dissertation is that
this approach helps to enable compilers that achieve these two goals. I explore
the performance of the approach by implementing a native code compiler with
encouraging results. This addresses the goal of high-performance code
generation. To verify correctness, I take advantage of the simplicity of the
approach to ease the proof burden, and implement a verified compiler using the
Coq proof assistant. These two implementations provide evidence that the \ce
machine is a powerful tool for implementing lazy functional programming
languages.

\section{Outline}

This dissertation is organized into six chapters. In this chapter, I provide
an introduction to the dissertation, including an outline of the structure,
instructions for access to artifacts and reproduction of results, a
retrospective, and an overview of the contributions.
Chapter~\ref{chap:background} provides necessary background for understanding
the dissertation, as well as further discussion of motivation. 
Chapter~\ref{chap:ce} defines and explains the \ce machine, in both big and
small-step semantics. Chapter~\ref{chap:cem} describes the implementation
of a native code compiler based on the \ce machine, and analyzes and discusses
its performance. Chapter~\ref{chap:verified} presents a verified compiler,
discussing the structure of the compiler and proofs. Finally, 
Chapter~\ref{chap:conclusion} discusses threats to validity, future work, and
conclusions. The appendices are used to give further implementation details,
both for the native code compiler (Appendix~\ref{chap:cem_appendix}) and the
verified compiler (Appendix~\ref{chap:coq_appendix}). In the case of the native
code compiler, the appendix shares some of the miscellaneous interesting
properties of the implementation. For the verified compiler, the purpose of the
appendix is to give the reader a fuller understanding of the structure and
definitions involved in the proofs.

\section{Reproducibility and Artifacts}

The implementations presented in this dissertation are available for download to
allow the reader to verify any claims made. All of the software is bundled as
a single tarball at \url{http://cs.unm.edu/\textasciitilde stelleg/cem.tgz}.
Instructions are included for building, running, and proof-checking the code.
For performance results, the hardware and operating system are listed in
Chapter~\ref{chap:cem}. In addition to the above tarball, each implementation
continues to be developed at \url{https://github.com/stelleg/cem} and
\url{https://github.com/stelleg/cem\_coq}. Finally, there is a simpler native
code compiler for pedagogical purposes, available at
\url{https://github.com/stelleg/cem\_pearl}. 

\section{On Laziness}

Because this work is focused on implementing call-by-need semantics, it is worth
spending some time discussing why we care about lazy evaluation. The focus here
is on high-level reasoning and opining, leaving a more technical coverage
of the topic for Section~\ref{sec:eval_strat}, which defines and contrasts
different evaluation strategies.

One easy argument for the importance of call-by-need is that it underlies the
widely used programming language Haskell. Technically, Haskell is a non-strict
language.  This implies that both call-by-name and call-by-need are valid
implementation strategies. In practice, there are some situations when one would
prefer call-by-name: when storing an intermediate value is more expensive than
re-computing it. This implies that in theory, Haskell could switch between
call-by-name and call-by-value depending on the situation. In practice,
implementations effectively always choose call-by-need, sometimes even
performing compile-time transformations that increase
sharing~\cite{jones96floating}.  

Even amongst the Haskell community, the advantages and disadvantages of
lazy evaluation are hotly debated. For example, there exist both strictness
annotations, and even strict-by-default variants of Haskell. There are real
reasons for preferring strict evaluation in some contexts. In particular,
reasoning about time and space requirements for lazy programs is notoriously
difficult. As a result, there are cases when the time and space requirements can
be surprisingly high.

The advantages of lazy semantics are most apparent when attempting to write
high-level, composable abstractions. This is a strong argument for code re-use
advantages in non-strict languages: by using laziness, one avoids work,
non-termination, and work-buffering where possible without additional programmer
effort~\cite{hughes1989functional}.

There are also well-known cases where composing lazy programs can result in
better asymptotics than strict composition. Consider the well-known example of
finding the minimal value in a list. 
\begin{verbatim}
  take 1 . sort
\end{verbatim}
With lazy semantics, this can result in an $O(n)$ time implementation, while the
strict implementation of compose will always result in an $O(n \log n)$
implementation (assuming an $O(n \log n)$ sort). This kind of asymptotic
improvement is a direct result of the efficiencies gained by avoiding eager
work. 

\section{Retrospective}

This section tells the story of how this dissertation came to be. The hope is to
convey to the reader some context for the structure and approach that the
dissertation takes. 

Everything started with an appreciation of lazy evaluation and a desire to know
how it works. Thus began investigation into how call-by-need semantics are
currently implemented. Inspired by presentations of simple call-by-need
approaches, such as the three instruction machine and the lazy Krivine machine,
as well as sophisticated approaches such as the STG machine, I was afflicted
with a nagging feeling that \emph{there must be a simpler, lazier way to
implement call-by-need}. After a lot of experimenting and thought, I finally
discovered the approach presented here. While I was optimistic about the
performance of a compiler, I was most excited by the \emph{simplicity} of the
approach. It was so easy to write a compiler! After a couple of failed attempts
at writing papers with the primary objective being to excite the reader about
the simplicity of the approach, I decided to instead focus on more concrete
properties. The first was performance: I hypothesized that the approach would
lead to cases where I could beat the state of the art. This was confirmed by
both a virtual machine and a native code compiler. It was also clear to me
that trying to build a high-performance compiler to outperform GHC on real-world
code was likely to fail, and I explicitly avoided making that a goal of the
dissertation. Instead, I focused on showing that there were cases that
outperform flat environments, leaving integrating shared and flat environments
for future work.

Once I had shown that there were performance benefits to the approach, I still
wanted to somehow use the simplicity of the approach for some concrete benefit.
Around this time, I became aware of the field of certified programming. I
realized I could use the simplicity of the approach to make formal reasoning
easier, and build the first verified compiler for call-by-need. This was
monumentally difficult. With very little training in formal reasoning, and no
training in dependent types and machine-checked proofs, it took a long time
working on my own to gain the skills to implement a verified compiler. Much of
the effort was due to being too ambitious. It is a relatively straightforward
thing to formalize and state theorems. Even when you are certain of the truth of
those theorems, it is an entirely different beast proving them in a
machine-checked logic. Every proof, definition, and theorem included in the
paper and in the Coq code was built on tens of aborted versions. Building the
verified compiler was the hardest thing I've ever done, by far.

Looking back, it would have been nice to have the two implementations be
combined into one. While nice in some respects, this combination is a daunting
task. Implementing a full native code compiler is a challenge in itself, but
specifying, implementing, and verifying a native code compiler is a massive
undertaking. CompCert, a verified compiler that compiles the lower level
language C, took multiple PhDs worth of work to
complete~\cite{leroy2012compcert}. That said, it would likely have worked to
verify and export into Haskell fragments of the native code compiler.  For
example, multiple times through the implementation process, the core language
was extended. Making such core changes in the presence of proofs of correctness
would make for a painful process, something that would have reduced the amount
of time available for experimenting with the implementation. Overall, I am
content with the approach of this dissertation: two separate compilers, with one
focused on performance and extensibility and the other focused on correctness. I
leave combining the two for future work, as I discuss further in
Section~\ref{sec:future}.

\section{Contributions}

There are three primary contributions of this dissertation.

\begin{itemize}
\item A novel technique for implementing call-by-need semantics using shared
environments, presented in Chapter~\ref{chap:ce}. The technique is formalized as
the \ce \\ machine, defined with both a big and small-step semantics.

\item A full native code compiler from a simple lazy functional language with
literals and primitive operations to x86\_64 machine code, presented in
Chapter~\ref{chap:cem}. The implementation follows naturally from the definition
of the \ce machine. I show that the compiler performs comparably to the state of
the art on a number of benchmarks.  This implementation and its analysis provide
evidence supporting the thesis that shared environment call-by-need has
performance benefits in some cases over existing approaches.

\item A verified compiler, presented in Chapter~\ref{chap:verified}, that
compiles call-by-need lambda calculus to a simple instruction machine, along
with a specification of correctness and a proof that the compiler adheres to
that specification. The compiler is implemented and the proofs checked in Coq,
mechanizing the \ce semantics in the process. This is the first verified
compiler of a call-by-need semantics. This implementation and mechanized proof
provides evidence for the thesis that the simplicity of \ce implementations
lends itself to formal verification 
\end{itemize}

Combined, these contributions support the core thesis of this dissertation: that
shared environment call-by-need has valuable contributions to make to the study
and implementation of call-by-need compilers. Smaller, more
implementation-specific contributions are enumerated in Chapters~\ref{chap:cem}
and~\ref{chap:verified}. 
