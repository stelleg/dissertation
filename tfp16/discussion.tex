\section{Discussion and Related Work} \label{sec:disc}

This section compares the compiler presented in this chapter with existing work,
and discusses areas for future work.

\subsection{Closure Representation}
Appel and Shao \cite{shao1994space} and Appel and Jim \cite{appel1988optimizing}
both cover the design space for closure representation, and develop an approach
called \emph{safely linked closures}. The approach uses flat closures when
there is no duplication, and links in a way that preserves liveness, to prevent
violation of the \emph{safe for space complexity} (SSC) rule
\cite{shao1994space}. While I do not address SSC or garbage collection in
general, understanding the relationship between SSC and shared environment
call-by-need is an interesting area for future work. In particular, hot
environments with no sharing could benefit greatly from replacing shared
structure with flat.

\subsection{Eval/Apply vs. Push/Enter}
Marlow and Peyton Jones describe two approaches to the implementation of
function application: eval/apply, where the function is evaluated and then
passed the necessary arguments, and push/enter, where the arguments are pushed
onto the stack and the function code is entered \cite{marlow2006making}. They
conclude that despite push/enter being a standard approach to lazy machines,
eval/apply performs better. While our current approach uses push/enter,
investigating whether eval/apply could be usefully implemented for a shared
environment machine like the $\mathcal{\mathcal{C} \mskip -4mu \mathcal{E}}$
machine is an interesting avenue for future work.

\subsection{Collapsed Markers}
Friedman et al.\ show how a machine can be designed to prevent multiple adjacent
update markers being pushed onto the stack \cite{lkm}.  This property is
desirable because multiple adjacent update markers are always updated with the
same value. They give examples showing that in some cases, these redundant
update markers can cause an otherwise constant-space stack to overflow.  They
implement an optimization that collapses update markers by adding a layer of
indirection between heap locations and closures. A similar approach,
but without the performance hit caused by an extra layer of indirection should
be possible, as follows: upon a variable dereference the $\mathcal{\mathcal{C}
\mskip -4mu \mathcal{E}}$ machine checks if the top of the stack is an update.
If it is, instead of pushing a redundant update marker onto the stack, the
machine replaces the closure in the heap at the marker location with an update
marker pointing to the location specified by the marker on the top of the stack.
Then, the variable dereference rule checks if there is an update marker instead
of a closure in the dereferenced cell, and if there is, then the value closure
pointed to by that update marker will be copied, overwriting the update marker.
This effectively makes the update mechanism lazier, only updating one marker
eagerly, and any equivalent markers on demand.  We leave this optimization for
future work.

\subsection{Register Allocation} \label{sec:alloc}
One advantage of flat environments is that register allocation is
straightforward \cite{appel1992compiling,jonesstg,terei2010llvm}. It is less
obvious how to do register allocation with the $\mathcal{\mathcal{C} \mskip
-4mu \mathcal{E}}$ machine.  

One possible approach that could work well with our shared environment approach
would be to only load free variables into registers that are statically known to
be needed. In other words, some environment variables may not be used, so only
those that will definitely be used should be loaded into registers when a
closure is entered, while the rest could be loaded on demand from memory.

\subsection{Characterizing Performance}
While we have provided a few benchmarks and some intuition for why the shared
environments would be preferable in some situations, we haven't really
characterized when programs will benefit from the approach.  This, along with
the joining of shared and flat environments, is left for future work. It will
likely require a combination of careful performance profiling, static analysis
tools, and a deeper understanding of the performance tradeoffs discussed in the
Chapter.

