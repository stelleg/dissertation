The strength of lazy functional programming languages is the freedom they give
the programmer to focus on correctness instead of operational details. In a
strict language, the programmer specifies what code will run, and when. In a
lazy language, the programmer only specifies what the result should be, leaving
what code should be executed up to the compiler. Because of this freedom from
operational concerns, there are two properties that lazy functional programmers
tend to have. First, they reason about the correctness of their code to a degree
seen almost nowhere else in the programming community \cite{totalhaskell}.
Second, they rely on compilers to generate efficient code in a way that
programmers of strict languages don't. Essentially, they are leaving more
operational decisions up to the compiler, and focusing their energy more on the
correctness of their code. 

It is for this reason that we compiler implementors must take great care in the
design of our compilers for lazy languages. We must build compilers that
generate efficient code: our programmers are relying particularly heavily on
our ability to generate efficient code. We also must ensure that our compilers
are correct: because the programmers are free to reason about the correctness of
their code, we must ensure that this additional reasoning is not invalidated by
bugs in the compiler.

This dissertation presents a tool for attaining these two goals: a novel
technique for implementing lazy semantics using shared environments, formalized
as the \ce machine. Essentially, the \ce machine repurposes shared environments
to share the results of computation. The thesis of this dissertation is that
this approach lends itself to compilers that achieve these two goals. I explore
the performance of the approach by implementing a native code compiler with
encouraging results. This addresses the goal of high performance code
generation. To verify correctness, I take advantage simplicity of the approach
to ease the proof burden, and implement a verified compiler using the Coq proof
assistant. These two implementations provide evidence that the \ce machine is a
powerful tool for implementing lazy functional programming languages.

\section{Outline}

This dissertation is organized into six chapters. In this chapter, I provide
an introduction to the dissertation, including an outline of the structure,
instructions for access to artifacts and reproduction of results, a
retrospective, and an overview of the contributions. In
Chapter~\ref{chap:background}, I provide necessary background for understanding
the dissertation, as well as further discussion of motivation. In
Chapter~\ref{chap:ce}, I define and explain the \ce machine, in both big and
small-step semantics. In Chapter~\ref{chap:cem}, I describe the implementation
of a native code compiler based on the \ce machine, and analyze and discuss its
performance. In Chapter~\ref{chap:verified}, I present a verified compiler,
discussing the structure of the compiler and proofs. Finally, in
Chapter~\ref{chap:conclusion}, I discuss threats to validity, future work, and
conclusions. The appendices are used to give further implementation details,
both for the native code compiler (Appendix~\ref{chap:cem_appendix}) and the
verified compiler (Appendix~\ref{chap:coq_appendix}). In the case of the native
code compiler, our hope is to share some of the other interesting properties of
the implementation.  For the verified compiler, the purpose of the appendix is
to give the reader a more full understanding of the structure and definitions
involved in the proofs.

\section{Reproducibility and Artifacts}

The implementations presented in this dissertation are available for download to
allow the reader to reproduce any claims made. All of the software is bundled as
a single tarball at \url{http://cs.unm.edu/\textasciitilde stelleg/cem.tgz}.
Instructions are included for building, running, and proof-checking the code.
For performance results, the hardware and operating system are listed in
Chapter~\ref{chap:cem}. In addition to the above tarball, each implementation has been
continued to be developed at \url{https://github.com/stelleg/cem} and
\url{https://github.com/stelleg/cem\_coq}. Finally, there is a simpler native
code compiler for pedagogical purposes available at
\url{https://github.com/stelleg/cem\_pearl}. 

\section{On Laziness}

Because this work is focused on implementing call-by-need semantics, it is worth
spending some time discussing why we care about lazy evaluation. The focus here
is on high level reasoning and opining, leaving a more technical coverage
of the topic for Section~\ref{sec:eval_strat}, which defines and contrasts
different evaluation strategies.

One easy argument for the importance of call-by-need is that it underlies the
widely used programming language Haskell. Technically, Haskell is a non-strict
language.  This implies that both call-by-name and call-by-need are valid
implementation strategies. In practice, there are some situations when one would
prefer call-by-name, namely, when storing an intermediate value is more
expensive than re-computing it. These cases are not limited to call-by-need;
call-by-value suffers the same issue.  This implies that in theory, Haskell
could switch between call-by-name and call-by-value depending on the situation.
In practice, implementations effectively always chooses call-by-need, sometimes
even performing compile-time transformations that increase sharing
\cite{jones96floating}.  

Even amongst the Haskell community, the advantages and disadvantages of
lazy evaluation are hotly debated. For example, there exist both strictness
annotations, and even strict-by-default variants of Haskell. There are real
reasons for preferring strict evaluation in some contexts. In particular,
reasoning about time and space requirements for lazy programs is notoriously
difficult.  

The advantages of lazy semantics often show up especially when attempting to
write high level, composable abstractions. This is a strong argument for code
re-use advantages in non-strict languages: by using laziness, one avoids work
and non-termination where possible without additional programmer effort.

There are also well-known cases of when composing lazy programs can result in
better asymptotics than strict composition. Consider the well-known example of
finding the minimal value in a list. 
\begin{verbatim}
  take 1 . sort
\end{verbatim}
With lazy semantics, this can result in an $O(n)$ implementation, while the
strict implementation of compose will always result in a $O(n \log n)$ 
implementation (assuming an $O(n \log n)$ sort). This kind of asymptotic
improvement is a direct result of the efficiencies gained by avoiding eager
work. 

Finally, and perhaps most importantly, \emph{I just like lazy semantics}. I like
the freedom from worrying about evaluation order it provides. I like that it
avoids work where possible. In my humble opinion, it is the ideal default
semantics for a functional programming language.   

\section{Retrospective}

This section tells the story of how this dissertation came to be. The hope is to
convey to the reader some context for the structure and approach that the
dissertation takes. 

Everything started with an appreciation of lazy evaluation and a desire to know
how it works. Thus began investigation into how call-by-need semantics are
currently implemented. Inspired by presentations of simple call-by-need
approaches, such as the three instruction machine and the lazy Krivine machine,
as well as sophisticated approaches such as the STG machine, I was afflicted
with a nagging feeling that \emph{there must be a simpler, lazier way to
implement call-by-need}. After a lot of experimenting and careful thought, I
finally discovered the approach presented here. While I was optimistic about the
performance of a compiler, I was most excited by the \emph{simplicity} of the
approach. It was so easy to write a compiler! After a couple failed attempts at
writing papers with the primary objective being to excite the reader about
the simplicity of the approach, I decided to instead focus on more concrete
properties. The first was performance: I hypothesized that the approach would
lead to cases where I could beat the state of the art. This was confirmed by
both a virtual machine the first native code compiler. It was also clear to me
that trying to build a high performance compiler to outperform GHC on real world
code was likely to fail, and I explicitly avoided making that a goal of the
dissertation. Instead, I focused on showing that there were cases that
outperform flat environments, with the hope that eventually 

Once I had shown that there were performance benefits to the approach, I still
wanted to somehow use the simplicity of the approach for some concrete benefit.
Around this time, I became aware of the field of certified programming. I
realized I could use the simplicity of the approach to make formal reasoning
easier, and build the first verified compiler for call-by-need. Saying this was
easier said than done is an understatement. With very little training in formal
reasoning, and no training in dependent types and machine-checked proofs, it
took a long time working on my own to gain the skills to implement a verified
compiler. Much of the effort was due to being too ambitious. It is a relatively
straightforward thing to formalize and state theorems. Even when you are certain
of the truth of those theorems, it is an entirely different beast proving them
in a machine-checked logic. Every proof, definition, and theorem included in the
paper and in the Coq code was built on tens of aborted versions. Building the
verified compiler was the hardest thing I've ever done, by far.

Looking back, it would have been nice to have the two implementations be
combined into one. While nice in some respects, this combination is a daunting
task. Implementing a full native code compiler is a challenge in itself, but
specifying, implementing, and verifying a native code compiler is a massive
undertaking. CompCert, a verified compiler that compiles the lower level
language C, took multiple PhDs worth of work to
complete~\cite{leroy2012compcert}. That said, it would likely have worked to
verify and export into Haskell fragments of the native code compiler.  For
example, multiple times through the implementation process, the core language
was extended. Making such core changes in the presence of proofs of correctness
would make for a painful process, something that would have slowed down valuable
time experimenting with the implementation. Overall, I am content with the
approach of this dissertation: two separate compilers, with one focused on
performance and extensibility and the other focused on correctness. I leave
combining the two for future work, as I discuss further in
Section~\ref{sec:future}.

\section{Contributions}

There are three primary contributions of this dissertation.

\begin{itemize}
\item A novel technique for implementing call-by-need semantics using shared
environments, presented in Chapter~\ref{chap:ce}. The technique is formalized as
the \ce \\ machine, defined with both a big and small-step semantics.

\item A full native code compiler from a simple lazy functional language with
literals and primitive operations to x86\_64 machine code, presented in
Chapter~\ref{chap:cem}. The implementation follows naturally from the definition
of the \ce machine. I show that the compiler performs comparably to the state of
the art on a number of benchmarks.  This implementation and its analysis provide
evidence supporting the thesis that shared environment call-by-need has
performance benefits in some cases over existing approaches.

\item A verified compiler, presented in Chapter~\ref{chap:verified}, that
compiles call-by-need lambda calculus to a simple instruction machine, along
with a specification of correctness and a proof that the compiler adheres to
that specification. The compiler is implemented and the proofs checked in Coq,
mechanizing the \ce semantics in the process. This is the first verified
compiler of a call-by-need semantics. This implementation and mechanized proof
provides evidence for the thesis that the simplicity of \ce implementations
lends itself to formal verification 
\end{itemize}

Combined, these contributions support the core thesis of this dissertation: that
shared environment call-by-need has valuable contributions to make to the study
and implementation of call-by-need compilers. Smaller, more
implementation-specific contributions are enumerated in Chapters~\ref{chap:cem}
and~\ref{chap:verified}. 
