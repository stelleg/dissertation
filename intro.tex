The strength of lazy functional programming languages is the freedom they
give the programmer to focus on correctness instead of operational details. As a
result, there are two properties that lazy functional programers tend to have.
First, they reason about the correctness of their code to a degree seen almost
nowhere else in the programming community. Second, they rely on the compilers
for these languages to generate efficient code in a way that programmers of
strict languages don't. Essentially, they are leaving more operational
decisions up to the compiler, and focusing their energy more on the
correctness of their code. 

It is for this reason that we compiler implementors must take great care in the
design of our compilers for lazy languages. We must have compilers that are
efficient: our programmers are relying particularly heavily on our ability to
generate efficient code. We also must ensure that our compilers are correct:
because the programmers are free to reason about the correctness of their code,
we must ensure that this additional reasoning is not invalidated by bugs in the
compiler. 

This dissertation presents a tool for attaining these two goals: a novel
technique for implementing lazy semantics using shared environments, formalized
as the \ce machine. Essentially, the \ce machine repurposes shared environments
to share the results of computation. The insight of this dissertation is that
this approach leads to a simple, efficient compiler. I exploit the efficiency of
the approach by implementing a native code compiler with good performance. This
addresses the goal of efficient compilation. To verify correctness, I take
advantage simplicity of the approach to ease the proof burden. These two
implementations provide ample evidence that the \ce machine has the properties
we want, and is therefore a powerful tool for implementing lazy functional
programming languages.

\section{Outline}

This dissertation is organized into six chapters. In this chapter, we provide
an introduction to the dissertation, including an outline of the structure,
instructions for access to artifacts and reproduction of results, and an
overview of the contributions. In Chapter~\ref{chap:background}, we provide
necessary background for understanding the dissertation, as well as further
discussion of motivation. In Chapter~\ref{chap:ce}  In Chapter~\ref{chap:cem},
we describe the implementation of a native code compiler based on the \ce
machine, and analyze and discuss its performance. In
Chapter~\ref{chap:verified}, we present the verified compiler, discussing the
structure of the compiler and proofs. Finally, in Chapter~\ref{chap:conclusion},
we discuss threats to validity, future work, and conclusions. We then use
appendices to give further implementation details, both for the native code
compiler and the verified compiler. In the case of the native code compiler, our
hope is to share some of the other interesting properties of the implementation.
For the verified compiler, the purpose of the appendix is to give the reader a
more full understanding of the structure and definitions involved in the proofs,
so that she may convince herself that they are correct.

\section{Reproducibility and Artifacts}

The implementations presented in this dissertation are available for download to
allow the reader to reproduce any claims made. All of the software is bundled as
a single tarball at \url{http://cs.unm.edu/\textasciitilde stelleg/cem.tgz}. 

\section{Call-by-Need}

Because this work is focused on implementing call-by-need semantics, it is worth
spending some time discussing why call-by-need semantics are important. One easy
argument is that call-by-need underlies the widely used programming language
Haskell. Technically, Haskell is a non-strict language. This implies that both
call-by-name and call-by-need are valid implementation strategies. In practice,
there are some situations when one would prefer call-by-name, namely, when
storing an intermediate value is more expensive than re-computing it. These
cases are not limited to call-by-need; call-by-value suffers the same issue.
This implies that in theory, Haskell could switch between call-by-name and
call-by-value depending on the situation. In practice, GHC effectively always
chooses call-by-need, sometimes performing transformations that increase
memoization \cite{jones96floating}.

Even amongst the Haskell community, the advantages and disadvantages of
non-strict evaluation are hotly debated. For example, there exist both
strictness annotations, and even strict-by-default variants of Haskell. There are
real reasons for preferring strict evaluation in some contexts. For example,
memory leaks due to naive attempts at freeing memory are a common issue with
lazy semantics. 

The advantages of non-strict semantics often show up when attempting to write
high level, higher-order abstractions. An example of this principle can be found
in the \texttt{lens} Haskell library. By using laziness everywhere possible, it
ensures that code can compose lenses without introducing nontermination. This is
a strong argument for code re-use advantages in non-strict languages: by using
laziness, one can prevent nontermination everywhere possible without any
additional work. 

In general, the purpose of this dissertation is not to convince the reader that
call-by-need is the ideal semantics. Instead, it takes as a given that it is one
worth spending time on. Finally, I will say that I believe there is a lot of
interesting potential for better understanding the time and space behavior of
non-strict languages. From substructural types to dependent types to control
flow analyses, the field of reasoning about time and space requirements for
higher order languages is still in its infancy, and I am excited to see it
combined with work like that presented here to give developers high confidence
in both their programs' correctness and their time and space requirements. 

\section{Retrospective}

As with any sufficiently large effort, there are places where, if I were to
start over, I would do things differently. A possible candidate for this
re-writing of history would be \emph{starting} with a verified compiler, so that
the native code compiler itself would be verified. While noble in nature, this
would be a daunting task. Implementing a full native code compiler is a
challenge in itself, but specifying, implementing, and verifying a native code
compiler is a massive undertaking. That said, it would likely have worked to
verify and export into Haskell fragments of the native code compiler. While this
would certainly have been feasible, it would have made modification more
difficult. For example, multiple times through the implementation process, the
core language was extended. Making such core changes in the presence of proofs
of correctness would make for a painful process, something that would have
slowed down valuable time experimenting with the implementation. Overall, I am
content with the approach of this dissertation: two separate compilers, with one
focused on performance and extensibility and the other focused on correctness. I
leave combining the two for future work, as I discuss further in
Section~\ref{sec:future}.

\section{Contributions}

There are two primary contributions of this dissertation, embodied in the
following two artifacts:

\begin{itemize}
\item A full native code compiler from a simple lazy functional language with
literals and primitive operations to x86\_64 machine code. We show that the
compiler performs comparably to the state of the art on a number of benchmarks.
This implementation and its analysis provide evidence supporting the thesis that
shared-environment call-by-need has novel efficiencies.

\item A verified compiler that compiles from lambda calculus to a simple
instruction machine, along with a specification of correctness and a proof that
the compiler adheres to that specification. The compiler is implemented and the
proofs checked in Coq. This is the first verified compiler of a call-by-need
semantics. This implementation and mechanized proof provides evidence for the
thesis that the simple compiler artifact that arises from shared-environment
call-by-need has benefits for formal reasoning.
\end{itemize}

Combined, these contributions support the core thesis of this dissertation: that
shared-environment call-by-need has valuable contributions to make to the study
and implementation of call-by-need compilers.
